{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929bb9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running RunTree Example ---\n",
      "--------------------\n",
      "\n",
      "--- Running Custom Tweak Example ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'function' object does not support the context manager protocol",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m topics = [\u001b[33m\"\u001b[39m\u001b[33mthe future of AI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mspace exploration\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mocean conservation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Use the 'traceable' context manager to wrap the entire operation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraceable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMulti-Topic Idea Generator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparent_run\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerated_ideas\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Each LLM call inside the loop will automatically become a child run\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# of our \"Multi-Topic Idea Generator\" run.\u001b[39;49;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'function' object does not support the context manager protocol"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith.run_helpers import traceable\n",
    "from langsmith.run_trees import RunTree\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. SETUP: Load environment variables\n",
    "# This ensures your OpenAI API key is available.\n",
    "load_dotenv()\n",
    "\n",
    "# Instantiate the model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# --- Key Example from Lesson 3: Using RunTree ---\n",
    "# This demonstrates manual control over tracing.\n",
    "def my_custom_logic(question: str):\n",
    "    \"\"\"A simple function that simulates a custom chain.\"\"\"\n",
    "    # Manually create a RunTree object to trace this function\n",
    "    rt = RunTree(\n",
    "        name=\"My Custom Chain\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question},\n",
    "    )\n",
    "    \n",
    "    # Manually create a child run for the LLM call\n",
    "    child_llm_run = rt.create_child(\n",
    "        name=\"My Custom LLM Call\",\n",
    "        run_type=\"llm\",\n",
    "        inputs={\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(question)\n",
    "    \n",
    "    # End the child run with its output\n",
    "    child_llm_run.end(outputs={\"generations\": [response.content]})\n",
    "    \n",
    "    # End the parent run with the final output\n",
    "    rt.end(outputs={\"answer\": response.content})\n",
    "    return response.content\n",
    "\n",
    "# Run the example function\n",
    "print(\"--- Running RunTree Example ---\")\n",
    "my_custom_logic(question=\"What is the key benefit of manual tracing with RunTree?\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "# --- Your Custom Tweak for Lesson 3 ---\n",
    "# This demonstrates using the 'traceable' context manager for a block of code.\n",
    "print(\"\\n--- Running Custom Tweak Example ---\")\n",
    "# A list of topics for our loop\n",
    "topics = [\"the future of AI\", \"space exploration\", \"ocean conservation\"]\n",
    "\n",
    "# Use the 'traceable' context manager to wrap the entire operation\n",
    "with traceable(run_type=\"chain\", name=\"Multi-Topic Idea Generator\") as parent_run:\n",
    "    generated_ideas = []\n",
    "    for topic in topics:\n",
    "        # Each LLM call inside the loop will automatically become a child run\n",
    "        # of our \"Multi-Topic Idea Generator\" run.\n",
    "        response = llm.invoke(f\"Generate a single, one-sentence startup idea about {topic}\")\n",
    "        print(f\"Idea for '{topic}': {response.content}\")\n",
    "        generated_ideas.append(response.content)\n",
    "    \n",
    "    # We can even add metadata to the parent run\n",
    "    parent_run.end(outputs={\"final_ideas\": generated_ideas})\n",
    "print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
